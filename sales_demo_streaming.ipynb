{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10bfd61f-3ba9-46f3-980e-df32208ef350",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Read data from Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0558b4-9323-4175-ac79-e96c200d8976",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d7d255c4-68c1-46d9-a6ee-2f9681a1a0cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, from_json, col, window, sum as _sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, LongType, BinaryType, BooleanType, TimestampNTZType\n",
    "import binascii\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d1e85d-068f-4ef1-8f92-0dfdd9a151f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b064dc3d-909f-4082-8065-e86355ce8245",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_path = \"s3://yout_bucket/\"\n",
    "# Configuraci√≥n de zona horaria\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91eadc92-4033-4c3a-af8a-f2ad26802e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7073dd25-4c24-4824-9702-8e7df7905aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# decode base64 price field\n",
    "def decode_base64_decimal(base64_bytes):\n",
    "    try:\n",
    "        price_decimal = int.from_bytes(base64_bytes, byteorder='big', signed=True) / 100.0\n",
    "        return price_decimal\n",
    "    except Exception:\n",
    "        return None\n",
    "      \n",
    "\n",
    "# Registre the function\n",
    "decode_base64_decimal_udf = udf(decode_base64_decimal, DoubleType())      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949718b5-0bf3-4c73-90a9-e9057762b9c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "86c567d0-43f1-43ec-9ec9-fa862ece5373",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka-Spark-Databricks-Demo-sales\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "kafka_brokers = \"<public-ip>:9092\"\n",
    "sales_topic = \"dbserver1.public.sales\"\n",
    "products_topic = \"dbserver1.public.products\"\n",
    "customers_topic = \"dbserver1.public.customers\"\n",
    "\n",
    "# Debezium kafka sales structure \n",
    "sales_schema = StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"sale_id\", IntegerType()),\n",
    "        StructField(\"customer_id\", IntegerType()),\n",
    "        StructField(\"product_id\", IntegerType()),\n",
    "        StructField(\"quantity\", IntegerType()),\n",
    "        StructField(\"sale_date\", LongType()),\n",
    "        StructField(\"created_at\", LongType()),\n",
    "        StructField(\"modified_at\", LongType())\n",
    "    ]), nullable=True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"sale_id\", IntegerType()),\n",
    "        StructField(\"customer_id\", IntegerType()),\n",
    "        StructField(\"product_id\", IntegerType()),\n",
    "        StructField(\"quantity\", IntegerType()),\n",
    "        StructField(\"sale_date\", LongType()),\n",
    "        StructField(\"created_at\", LongType()),\n",
    "        StructField(\"modified_at\", LongType())\n",
    "    ])),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType()),\n",
    "        StructField(\"connector\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"ts_ms\", LongType()),\n",
    "        StructField(\"snapshot\", StringType()),\n",
    "        StructField(\"db\", StringType()),\n",
    "        StructField(\"sequence\", StringType()),\n",
    "        StructField(\"schema\", StringType()),\n",
    "        StructField(\"table\", StringType()),\n",
    "        StructField(\"txId\", LongType()),\n",
    "        StructField(\"lsn\", LongType()),\n",
    "        StructField(\"xmin\", LongType(), nullable=True)\n",
    "    ])),\n",
    "    StructField(\"op\", StringType()),\n",
    "    StructField(\"ts_ms\", LongType()),\n",
    "    StructField(\"transaction\", StructType([\n",
    "        StructField(\"id\", StringType(), nullable=True),\n",
    "        StructField(\"total_order\", LongType(), nullable=True),\n",
    "        StructField(\"data_collection_order\", LongType(), nullable=True)\n",
    "    ]), nullable=True)\n",
    "])\n",
    "\n",
    "# Debezium kafka products structure\n",
    "products_schema = StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"product_id\", IntegerType()),\n",
    "        StructField(\"product_name\", StringType()),\n",
    "        StructField(\"description\", StringType()),\n",
    "        StructField(\"price\", BinaryType()),\n",
    "        StructField(\"created_at\", LongType()),\n",
    "        StructField(\"modified_at\", LongType())\n",
    "    ]), nullable=True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"product_id\", IntegerType()),\n",
    "        StructField(\"product_name\", StringType()),\n",
    "        StructField(\"description\", StringType()),\n",
    "        StructField(\"price\", BinaryType()),\n",
    "        StructField(\"created_at\", LongType()),\n",
    "        StructField(\"modified_at\", LongType())\n",
    "    ])),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType()),\n",
    "        StructField(\"connector\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"ts_ms\", LongType()),\n",
    "        StructField(\"snapshot\", StringType()),\n",
    "        StructField(\"db\", StringType()),\n",
    "        StructField(\"sequence\", StringType()),\n",
    "        StructField(\"schema\", StringType()),\n",
    "        StructField(\"table\", StringType()),\n",
    "        StructField(\"txId\", LongType()),\n",
    "        StructField(\"lsn\", LongType()),\n",
    "        StructField(\"xmin\", LongType(), nullable=True)\n",
    "    ])),\n",
    "    StructField(\"op\", StringType()),\n",
    "    StructField(\"ts_ms\", LongType()),\n",
    "    StructField(\"transaction\", StructType([\n",
    "        StructField(\"id\", StringType(), nullable=True),\n",
    "        StructField(\"total_order\", LongType(), nullable=True),\n",
    "        StructField(\"data_collection_order\", LongType(), nullable=True)\n",
    "    ]), nullable=True)\n",
    "])\n",
    "\n",
    "# Debezium kafka customers structure \n",
    "customers_schema = StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"customer_id\", IntegerType()),\n",
    "        StructField(\"first_name\", StringType()),\n",
    "        StructField(\"last_name\", StringType()),\n",
    "        StructField(\"email\", StringType()),\n",
    "        StructField(\"phone\", StringType()),\n",
    "        StructField(\"created_at\", LongType()),\n",
    "        StructField(\"modified_at\", LongType())\n",
    "    ]), nullable=True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"customer_id\", IntegerType()),\n",
    "        StructField(\"first_name\", StringType()),\n",
    "        StructField(\"last_name\", StringType()),\n",
    "        StructField(\"email\", StringType()),\n",
    "        StructField(\"phone\", StringType()),\n",
    "        StructField(\"created_at\", LongType()),\n",
    "        StructField(\"modified_at\", LongType(), nullable=True)\n",
    "    ])),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType()),\n",
    "        StructField(\"connector\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"ts_ms\", LongType()),\n",
    "        StructField(\"snapshot\", StringType()),\n",
    "        StructField(\"db\", StringType()),\n",
    "        StructField(\"sequence\", StringType()),\n",
    "        StructField(\"schema\", StringType()),\n",
    "        StructField(\"table\", StringType()),\n",
    "        StructField(\"txId\", LongType()),\n",
    "        StructField(\"lsn\", LongType()),\n",
    "        StructField(\"xmin\", LongType(), nullable=True)\n",
    "    ])),\n",
    "    StructField(\"op\", StringType()),\n",
    "    StructField(\"ts_ms\", LongType()),\n",
    "    StructField(\"transaction\", StructType([\n",
    "        StructField(\"id\", StringType(), nullable=True),\n",
    "        StructField(\"total_order\", LongType(), nullable=True),\n",
    "        StructField(\"data_collection_order\", LongType(), nullable=True)\n",
    "    ]), nullable=True)\n",
    "])\n",
    "\n",
    "# Read sales topic from kafka \n",
    "sales_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", sales_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Read products topic from kafka\n",
    "products_df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", products_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Read customers topic from kafka \n",
    "customers_df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", customers_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Crear una vista temporal para productos\n",
    "products_df.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f07ee8-de8f-4d22-a959-0a934735fe65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parse dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a0cf15ba-cdbc-4052-93ec-930f04c7cabe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse dataframe\n",
    "sales_parsed_df = sales_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", sales_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "products_parsed_df = products_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", products_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "customers_parsed_df = customers_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", customers_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "sales_after_df = sales_parsed_df.select(\"after.*\")\n",
    "products_after_df = products_parsed_df.select(\"after.*\")\n",
    "customers_after_df = customers_parsed_df.select(\"after.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d334a2-cfe8-4e3c-a580-c7a28e32f398",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply columns transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5149d123-5811-4a91-b9be-9d65e0659cc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_final_df = sales_after_df.withColumn(\n",
    "    \"sale_date\", (col(\"sale_date\") / 1000000).cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"created_at\", (col(\"created_at\") / 1000000).cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"modified_at\", (col(\"modified_at\") / 1000000).cast(TimestampType())\n",
    ")\n",
    "\n",
    "products_final_df = products_after_df.withColumn(\n",
    "    \"price\", decode_base64_decimal_udf(col(\"price\"))\n",
    ").withColumn(\n",
    "    \"created_at\", (col(\"created_at\") / 1000000).cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"modified_at\", (col(\"modified_at\") / 1000000).cast(TimestampType())\n",
    ")\n",
    "\n",
    "customers_final_df = customers_after_df.withColumn(\n",
    "    \"created_at\", (col(\"created_at\") / 1000000).cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"modified_at\", (col(\"modified_at\") / 1000000).cast(TimestampType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88712029-86fd-4038-b6ef-b6263172e865",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Write batch delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "01b208dd-3b03-41e6-a41c-3fcd666afc81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_delta_path = storage_path + \"demo_sales/products\"\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo_sales.products\n",
    "USING delta\n",
    "LOCATION '{products_delta_path}'\n",
    "\"\"\")\n",
    "\n",
    "products_final_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(products_delta_path)\n",
    "\n",
    "customers_delta_path = storage_path + \"demo_sales/customers\"\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo_sales.customers\n",
    "USING delta\n",
    "LOCATION '{customers_delta_path}'\n",
    "\"\"\")\n",
    "\n",
    "customers_final_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(customers_delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e7306a-f130-469c-a053-9ffc4ebdc2a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Stream Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f8909-b4fe-4f94-880d-d81e461222b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a temp view from products\n",
    "products_final_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Join dataframe\n",
    "joined_df = sales_final_df.join(spark.table(\"products\"), sales_final_df.product_id == col(\"products.product_id\")) \\\n",
    "    .select(\n",
    "        sales_final_df.sale_date,\n",
    "        sales_final_df.quantity,\n",
    "        col(\"products.price\")\n",
    "    )\n",
    "\n",
    "# Create a window processing\n",
    "windowed_df = joined_df \\\n",
    "    .withWatermark(\"sale_date\", \"1 hour\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"sale_date\"), \"1 hour\"),\n",
    "        expr(\"EXTRACT(YEAR FROM sale_date) AS anio\"),\n",
    "        expr(\"EXTRACT(MONTH FROM sale_date) AS mes\"),\n",
    "        expr(\"EXTRACT(WEEK FROM sale_date) AS semana\"),\n",
    "        expr(\"EXTRACT(DAY FROM sale_date) AS dia\"),\n",
    "        expr(\"EXTRACT(HOUR FROM sale_date) AS hora\")\n",
    "    ) \\\n",
    "    .agg(_sum(col(\"quantity\") * col(\"price\")).alias(\"venta_neta\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b6ac50-7590-479a-9a20-59ed0b444d33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Write Streaming Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417bf597-c9bb-40c8-9264-ebe27e9ccfc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_delta_path = storage_path + \"demo_sales/sales\"\n",
    "checkpoint_sales_delta_path = storage_path + \"demo_sales/checkpoint/sales\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo_sales.sales\n",
    "USING delta\n",
    "LOCATION '{sales_delta_path}'\n",
    "\"\"\")\n",
    "\n",
    "# Write data in append mode\n",
    "sales_query = sales_final_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_sales_delta_path) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(sales_delta_path)\n",
    "\n",
    "dwt_sales_delta_path = storage_path + \"demo_sales/dwt_sales_1\"\n",
    "checkpoint_dwt_sales_delta_path = storage_path + \"demo_sales/checkpoint/dwt_sales_1\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo_sales.dwt_sales_1\n",
    "USING delta\n",
    "LOCATION '{dwt_sales_delta_path}'\n",
    "\"\"\")\n",
    "\n",
    "# Write data in complete mode\n",
    "dwt_sales_query = windowed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dwt_sales_delta_path) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(dwt_sales_delta_path)\n",
    "\n",
    "dwt_sales_query.awaitTermination()\n",
    "sales_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3123831721142924,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "sales-demo-streamming - 2024-06-30",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
